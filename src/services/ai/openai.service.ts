import OpenAI from 'openai';
import { encoding_for_model } from 'tiktoken';
import { config } from '../../config';
import { z } from 'zod';

// Types
export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface EmbeddingResponse {
  embedding: number[];
  tokens: number;
}

export interface CompletionOptions {
  temperature?: number;
  maxTokens?: number;
  topP?: number;
  frequencyPenalty?: number;
  presencePenalty?: number;
  stop?: string[];
}

// Configuration from environment
const AI_CONFIG = {
  MODEL: process.env.OPENAI_MODEL || 'gpt-4o-mini',
  EMBEDDING_MODEL: process.env.OPENAI_EMBEDDING_MODEL || 'text-embedding-3-small',
  MAX_TOKENS: parseInt(process.env.OPENAI_MAX_TOKENS || '1000'),
  TEMPERATURE: parseFloat(process.env.OPENAI_TEMPERATURE || '0.7'),
  MONTHLY_LIMIT: parseFloat(process.env.OPENAI_MONTHLY_LIMIT || '10'),
};

// Service class
export class OpenAIService {
  private client: OpenAI | null = null;
  private encoder: any;
  private totalCost: number = 0;
  
  constructor() {
    // Initialize OpenAI client only if API key exists
    if (config.OPENAI_API_KEY) {
      this.client = new OpenAI({
        apiKey: config.OPENAI_API_KEY,
      });
      console.log('âœ… OpenAI client initialized with model:', AI_CONFIG.MODEL);
    } else {
      console.warn('âš ï¸ OpenAI API key not configured - using mock mode');
    }
    
    // Initialize tokenizer for cost calculation
    try {
      this.encoder = encoding_for_model('gpt-3.5-turbo');
    } catch {
      console.warn('âš ï¸ Tokenizer initialization failed');
    }
  }
  
  /**
   * ØªÙ†Ø¸ÙŠÙ JSON Ù…Ù† markdown blocks - Ø¯Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù‡Ù…Ø©
   */
  private cleanJsonResponse(text: string): string {
    // Ø¥Ø²Ø§Ù„Ø© markdown code blocks
    let cleaned = text;
    
    // Ø¥Ø²Ø§Ù„Ø© ```json Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ùˆ``` Ù…Ù† Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
    cleaned = cleaned.replace(/^```json\s*\n?/i, '');
    cleaned = cleaned.replace(/^```\s*\n?/i, '');
    cleaned = cleaned.replace(/\n?```\s*$/i, '');
    
    // Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ markdown blocks Ø£Ø®Ø±Ù‰
    cleaned = cleaned.replace(/```[a-z]*\n?/gi, '');
    
    // ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    cleaned = cleaned.trim();
    
    return cleaned;
  }
  
  /**
   * Chat completion expecting JSON response - Ø¯Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù€ Multi-Agent
   */
  async chatJSON(
    messages: ChatMessage[],
    options: CompletionOptions = {}
  ): Promise<any> {
    const response = await this.chat(messages, {
      ...options,
      temperature: options.temperature ?? 0.5, // Ø£Ù‚Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ù„Ù€ JSON
    });
    
    try {
      // ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù€ JSON Ø£ÙˆÙ„Ø§Ù‹
      const cleaned = this.cleanJsonResponse(response);
      
      // Ù…Ø­Ø§ÙˆÙ„Ø© parse
      return JSON.parse(cleaned);
    } catch (error) {
      console.error('âŒ JSON parsing failed even after cleaning');
      console.log('Raw response (first 200 chars):', response.substring(0, 200));
      console.log('Cleaned response (first 200 chars):', this.cleanJsonResponse(response).substring(0, 200));
      
      // Return empty object as fallback
      return {};
    }
  }
  
  /**
   * Calculate cost for API usage
   */
  private calculateCost(model: string, inputTokens: number, outputTokens: number): number {
    const prices: Record<string, { input: number; output: number }> = {
      'gpt-4o-mini': { input: 0.00015, output: 0.0006 }, // per 1K tokens
      'gpt-3.5-turbo': { input: 0.0005, output: 0.0015 },
      'text-embedding-3-small': { input: 0.00002, output: 0 },
    };
    
    const modelPrices = prices[model] || prices['gpt-4o-mini'];
    const cost = (inputTokens * modelPrices.input + outputTokens * modelPrices.output) / 1000;
    
    this.totalCost += cost;
    
    // Check monthly limit
    if (this.totalCost > AI_CONFIG.MONTHLY_LIMIT) {
      console.warn(`âš ï¸ Monthly limit reached: $${this.totalCost.toFixed(2)}`);
    }
    
    return cost;
  }
  
  /**
   * Generate text embedding
   */
  async generateEmbedding(text: string): Promise<EmbeddingResponse> {
    // If no client, return mock embedding
    if (!this.client) {
      return {
        embedding: Array(1536).fill(0).map(() => Math.random()),
        tokens: Math.ceil(text.length / 4),
      };
    }
    
    try {
      const response = await this.client.embeddings.create({
        model: AI_CONFIG.EMBEDDING_MODEL,
        input: text,
      });
      
      // Calculate cost
      const tokens = response.usage?.total_tokens || 0;
      const cost = this.calculateCost(AI_CONFIG.EMBEDDING_MODEL, tokens, 0);
      console.log(`ğŸ’° Embedding cost: $${cost.toFixed(4)}`);
      
      return {
        embedding: response.data[0].embedding,
        tokens,
      };
    } catch (error: any) {
      console.error('âŒ Embedding generation failed:', error.message);
      
      // Return mock on error
      return {
        embedding: Array(1536).fill(0).map(() => Math.random()),
        tokens: Math.ceil(text.length / 4),
      };
    }
  }
  
  /**
   * Generate embeddings for multiple texts
   */
  async generateEmbeddings(texts: string[]): Promise<EmbeddingResponse[]> {
    const batchSize = 100;
    const results: EmbeddingResponse[] = [];
    
    for (let i = 0; i < texts.length; i += batchSize) {
      const batch = texts.slice(i, i + batchSize);
      
      if (!this.client) {
        // Mock embeddings
        results.push(...batch.map(text => ({
          embedding: Array(1536).fill(0).map(() => Math.random()),
          tokens: Math.ceil(text.length / 4),
        })));
        continue;
      }
      
      try {
        const response = await this.client.embeddings.create({
          model: AI_CONFIG.EMBEDDING_MODEL,
          input: batch,
        });
        
        results.push(...response.data.map((item, idx) => ({
          embedding: item.embedding,
          tokens: Math.floor((batch[idx]?.length || 0) / 4),
        })));
        
      } catch (error: any) {
        console.error('âŒ Batch embedding failed:', error.message);
        // Add mock embeddings for failed batch
        results.push(...batch.map(text => ({
          embedding: Array(1536).fill(0).map(() => Math.random()),
          tokens: Math.ceil(text.length / 4),
        })));
      }
    }
    
    return results;
  }
  
  /**
   * Chat completion with improved error handling
   */
  async chat(
    messages: ChatMessage[],
    options: CompletionOptions = {}
  ): Promise<string> {
    // If no client, check if we should mock
    if (!this.client) {
      return this.getMockResponse(messages[messages.length - 1].content);
    }
    
    try {
      // Count input tokens
      const inputTokens = messages.reduce((sum, msg) => 
        sum + this.countTokens(msg.content), 0
      );
      
      const response = await this.client.chat.completions.create({
        model: AI_CONFIG.MODEL,
        messages,
        temperature: options.temperature ?? AI_CONFIG.TEMPERATURE,
        max_tokens: options.maxTokens ?? AI_CONFIG.MAX_TOKENS,
        top_p: options.topP ?? 1,
        frequency_penalty: options.frequencyPenalty ?? 0,
        presence_penalty: options.presencePenalty ?? 0,
        stop: options.stop,
      });
      
      const content = response.choices[0]?.message?.content || '';
      const outputTokens = this.countTokens(content);
      
      // Calculate and log cost
      const cost = this.calculateCost(AI_CONFIG.MODEL, inputTokens, outputTokens);
      console.log(`ğŸ’° Chat cost: $${cost.toFixed(4)} | Total: $${this.totalCost.toFixed(2)}`);
      
      return content;
    } catch (error: any) {
      console.error('âŒ Chat completion failed:', error.message);
      
      // Fallback to mock response
      return this.getMockResponse(messages[messages.length - 1].content);
    }
  }
  
  /**
   * Stream chat completion
   */
  async *chatStream(
    messages: ChatMessage[],
    options: CompletionOptions = {}
  ): AsyncGenerator<string, void, unknown> {
    if (!this.client) {
      yield this.getMockResponse(messages[messages.length - 1].content);
      return;
    }
    
    try {
      const stream = await this.client.chat.completions.create({
        model: AI_CONFIG.MODEL,
        messages,
        temperature: options.temperature ?? AI_CONFIG.TEMPERATURE,
        max_tokens: options.maxTokens ?? AI_CONFIG.MAX_TOKENS,
        stream: true,
      });
      
      for await (const chunk of stream) {
        yield chunk.choices[0]?.delta?.content || '';
      }
    } catch (error: any) {
      console.error('âŒ Chat stream failed:', error.message);
      yield this.getMockResponse(messages[messages.length - 1].content);
    }
  }
  
  /**
   * Generate structured output with better error handling
   */
  async generateStructured<T>(
    prompt: string,
    schema: z.ZodSchema<T>,
    options: CompletionOptions = {}
  ): Promise<T> {
    const systemPrompt = `Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØ±Ø¯ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ù€ JSON ØµØ§Ù„Ø­.
Ø¥Ø¬Ø§Ø¨ØªÙƒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ·Ø§Ø¨Ù‚ Ù‡Ø°Ø§ Ø§Ù„Ù€ schema Ø¨Ø§Ù„Ø¶Ø¨Ø·:
${JSON.stringify(schema._def, null, 2)}

Ø±Ø¯ ÙÙ‚Ø· Ø¨Ù€ JSON ØµØ§Ù„Ø­ØŒ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ Ø£Ùˆ markdown.`;
    
    const response = await this.chat(
      [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: prompt },
      ],
      { ...options, temperature: 0.3 }
    );
    
    // Clean response from markdown if exists
    const cleanResponse = response
      .replace(/```json\n?/g, '')
      .replace(/```\n?/g, '')
      .trim();
    
    try {
      const parsed = JSON.parse(cleanResponse);
      return schema.parse(parsed);
    } catch (error) {
      console.error('âŒ Failed to parse structured response:', error);
      // Return a default based on schema
      throw new Error('Invalid structured response from AI');
    }
  }
  
  /**
   * Count tokens in text
   */
  countTokens(text: string): number {
    if (!this.encoder) {
      // Rough estimate for Arabic: 1 token â‰ˆ 3 characters
      return Math.ceil(text.length / 3);
    }
    
    try {
      const tokens = this.encoder.encode(text);
      return tokens.length;
    } catch {
      return Math.ceil(text.length / 3);
    }
  }
  
  /**
   * Split text into chunks by token count
   */
  splitTextByTokens(text: string, maxTokens: number = 1000): string[] {
    const chunks: string[] = [];
    const sentences = text.split(/[.!?]\s+/);
    let currentChunk = '';
    let currentTokens = 0;
    
    for (const sentence of sentences) {
      const sentenceTokens = this.countTokens(sentence);
      
      if (currentTokens + sentenceTokens > maxTokens && currentChunk) {
        chunks.push(currentChunk.trim());
        currentChunk = sentence;
        currentTokens = sentenceTokens;
      } else {
        currentChunk += (currentChunk ? '. ' : '') + sentence;
        currentTokens += sentenceTokens;
      }
    }
    
    if (currentChunk) {
      chunks.push(currentChunk.trim());
    }
    
    return chunks;
  }
  
  /**
   * Get mock response for testing
   */
  private getMockResponse(message: string): string {
    const mockResponses: Record<string, string> = {
      'Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯': 'Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªÙŠ Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙÙŠ Ø§Ù„Ø¹Ø¯: 1ØŒ 2ØŒ 3ØŒ 4ØŒ ÙˆÙ‡ÙƒØ°Ø§.',
      'Ø§Ù„ÙƒØ³ÙˆØ±': 'Ø§Ù„ÙƒØ³Ø± Ù‡Ùˆ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„ÙƒÙ„ØŒ Ù…Ø«Ù„ 1/2 (Ù†ØµÙ) Ø£Ùˆ 1/4 (Ø±Ø¨Ø¹).',
      'Ø§Ù„Ø¶Ø±Ø¨': 'Ø§Ù„Ø¶Ø±Ø¨ Ù‡Ùˆ Ø¬Ù…Ø¹ Ù…ØªÙƒØ±Ø±. Ù…Ø«Ù„Ø§Ù‹: 3 Ã— 4 ÙŠØ¹Ù†ÙŠ 3 + 3 + 3 + 3 = 12',
      'default': 'Ø´ÙƒØ±Ø§Ù‹ Ù„Ø³Ø¤Ø§Ù„Ùƒ! Ø£Ù†Ø§ ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠÙØ±Ø¬Ù‰ Ø¥Ø¶Ø§ÙØ© OpenAI API key Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø§Øª ÙƒØ§Ù…Ù„Ø©.',
    };
    
    // Find matching response
    for (const [key, value] of Object.entries(mockResponses)) {
      if (message.includes(key)) {
        return value;
      }
    }
    
    return mockResponses.default;
  }
  
  /**
   * Get usage statistics
   */
  getUsageStats() {
    return {
      totalCost: this.totalCost.toFixed(2),
      monthlyLimit: AI_CONFIG.MONTHLY_LIMIT,
      remainingBudget: (AI_CONFIG.MONTHLY_LIMIT - this.totalCost).toFixed(2),
      model: AI_CONFIG.MODEL,
    };
  }
}

// Export singleton instance
export const openAIService = new OpenAIService();